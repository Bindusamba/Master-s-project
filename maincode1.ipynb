{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bindusamba/Documents/GitHub/Master-s-project/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense, Input, GaussianNoise\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files\n",
    "gene_df = pd.read_csv('/Users/bindusamba/Documents/GitHub/Master-s-project/csv/Gene.csv')\n",
    "mainsheet_df = pd.read_csv('/Users/bindusamba/Documents/GitHub/Master-s-project/csv/Mainset.csv')\n",
    "product_df = pd.read_csv('/Users/bindusamba/Documents/GitHub/Master-s-project/csv/Product.csv')\n",
    "promoter_df = pd.read_csv('/Users/bindusamba/Documents/GitHub/Master-s-project/csv/Promoter.csv')\n",
    "species_df = pd.read_csv('/Users/bindusamba/Documents/GitHub/Master-s-project/csv/Species.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene Data:\n",
      "   GeneID GeneName GeneSynonyms  GenePositionStart  GenePositionEnd  \\\n",
      "0       1     acrD  b2470, yffA            2585617          2588730   \n",
      "1       2     actP  b4067, yjcG            4282925          4281276   \n",
      "2       3     hisP        b2306            2422531          2421758   \n",
      "3       4    hisM         b2307            2423255          2422539   \n",
      "4       5    hisQ         b2308            2423938          2423252   \n",
      "\n",
      "  GeneStrand  GeneNCBI  ProductID  \n",
      "0          +  945464.0          1  \n",
      "1          -  948575.0          2  \n",
      "2          +  946789.0          3  \n",
      "3          -  946790.0          4  \n",
      "4          -  947235.0          5  \n",
      "\n",
      "MainSheet Data:\n",
      "   MainsetID  PromoterID  GeneID  SpeciesID\n",
      "0          1           1       1          1\n",
      "1          2           2       2          1\n",
      "2          3           3       3          1\n",
      "3          4           3       4          1\n",
      "4          5           3       5          1\n",
      "\n",
      "Product Data:\n",
      "   ProductID ProductType                                        ProductName  \\\n",
      "0          0     Protein                                                NaN   \n",
      "1          1     Protein  AcrD, subunit of AcrAD-TolC multidrug efflux t...   \n",
      "2          2     Protein                      Cation/acetate symporter ActP   \n",
      "3          3     Protein       Histidine transport ATP-binding protein HisP   \n",
      "4          4     Protein   Histidine transport system permease protein HisM   \n",
      "\n",
      "  UniProt  \n",
      "0     NaN  \n",
      "1  C6EKP2  \n",
      "2  P32705  \n",
      "3  P07109  \n",
      "4  P0AEU3  \n",
      "\n",
      "Promoter Data:\n",
      "   PromoterID PromoterName  PromoterPosition PromoterStrand  \\\n",
      "0           1       acrDp2           2585572              +   \n",
      "1           2        actPp           4282958              -   \n",
      "2           3        argTp           2425871              +   \n",
      "3           4         aslB           3980804              +   \n",
      "4           5       astCp2           1830068              +   \n",
      "\n",
      "                                    PromoterSequence PromoterMotifa  \\\n",
      "0  atttacattaactcctttttttctccacgattggctcgtaccttgc...         tggctc   \n",
      "1  gatctcctttgttctcaccggtatctacatctggcgggcgaacggc...         tggcgg   \n",
      "2  tatttaacgttgaatgttactgttgtcgtcaagatggcataagacc...         tggcat   \n",
      "3  aaaaagcagtatttcggcgagtagcgcagcttggtagcgcaactgg...         tggtag   \n",
      "4  tagcctccgccgtttatgcacttttatcactggctggcacgaaccc...         tggcac   \n",
      "\n",
      "  PromoterMotifb                                   BriefDescription  \\\n",
      "0          tgccg  This promoter was identified by microarray ass...   \n",
      "1          ggcga  Transcription of acs may be negatively regulat...   \n",
      "2          tgcat  We assigned a putative transcription start sit...   \n",
      "3          tggtt  This promoter was identified by microarray ass...   \n",
      "4          tgcaa  The expression of this promoter is induced in ...   \n",
      "\n",
      "                   Reference  \n",
      "0                   14529615  \n",
      "1          14563880,10894724  \n",
      "2  10536136,11528004,3118148  \n",
      "3                   19969540  \n",
      "4           9696780,12003934  \n",
      "\n",
      "Species Data:\n",
      "   SpeciesID               SpeciesName  \\\n",
      "0          1          Escherichia coli   \n",
      "1          2        Ralstonia eutropha   \n",
      "2          3   Azospirillum brasilense   \n",
      "3          4  Azorhizobium caulinodans   \n",
      "4          5    Azotobacter vinelandii   \n",
      "\n",
      "                             SpeciesSynonyms  \n",
      "0  Escherichia coli str. K-12 substr. MG1655  \n",
      "1                     Ralstonia eutropha H16  \n",
      "2              Azospirillum brasilense Sp245  \n",
      "3           Azorhizobium caulinodans ORS 571  \n",
      "4                 Azotobacter vinelandii CA6  \n"
     ]
    }
   ],
   "source": [
    "# Display first few rows of each dataset\n",
    "print(\"Gene Data:\")\n",
    "print(gene_df.head())\n",
    "\n",
    "print(\"\\nMainSheet Data:\")\n",
    "print(mainsheet_df.head())\n",
    "\n",
    "print(\"\\nProduct Data:\")\n",
    "print(product_df.head())\n",
    "\n",
    "print(\"\\nPromoter Data:\")\n",
    "print(promoter_df.head())\n",
    "\n",
    "print(\"\\nSpecies Data:\")\n",
    "print(species_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1: atttacattaactcctttttttctccacgattggctcgtaccttgccgctacagtgaagcAagtcaagcctacaacgatac (Length: 81)\n",
      "Sequence 2: gatctcctttgttctcaccggtatctacatctggcgggcgaacggcgaattcgaccgtctTaataatgaagtcctgcatga (Length: 81)\n",
      "Sequence 3: tatttaacgttgaatgttactgttgtcgtcaagatggcataagacctgcatgaaagagccTgcaaacacacaacacaatac (Length: 81)\n",
      "Sequence 4: aaaaagcagtatttcggcgagtagcgcagcttggtagcgcaactggtttgggaccagtggGtcggaggttcgaatcctctc (Length: 81)\n",
      "Sequence 5: tagcctccgccgtttatgcacttttatcactggctggcacgaaccctgcaatctacatttAcagcgcaaacattacttatt (Length: 81)\n"
     ]
    }
   ],
   "source": [
    "# Drop NaN values and ensure sequences are strings\n",
    "promoter_df = promoter_df.dropna(subset=[\"PromoterSequence\"])\n",
    "promoter_df[\"PromoterSequence\"] = promoter_df[\"PromoterSequence\"].astype(str)\n",
    "\n",
    "# Extract promoter sequences\n",
    "promoter_sequences = promoter_df[\"PromoterSequence\"].values\n",
    "\n",
    "# Check sequence lengths\n",
    "sequence_lengths = [len(seq) for seq in promoter_sequences]\n",
    "\n",
    "# Print first 5 sequences and their lengths\n",
    "for i in range(5):\n",
    "    print(f\"Sequence {i+1}: {promoter_sequences[i]} (Length: {sequence_lengths[i]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ One-hot encoding added as a new column 'OneHotEncoded' in 'promoter.csv'\n",
      "First 5 rows of the updated DataFrame:\n",
      "   PromoterID PromoterName  PromoterPosition PromoterStrand  \\\n",
      "0           1       acrDp2           2585572              +   \n",
      "1           2        actPp           4282958              -   \n",
      "2           3        argTp           2425871              +   \n",
      "3           4         aslB           3980804              +   \n",
      "4           5       astCp2           1830068              +   \n",
      "\n",
      "                                    PromoterSequence PromoterMotifa  \\\n",
      "0  ATTTACATTAACTCCTTTTTTTCTCCACGATTGGCTCGTACCTTGC...         tggctc   \n",
      "1  GATCTCCTTTGTTCTCACCGGTATCTACATCTGGCGGGCGAACGGC...         tggcgg   \n",
      "2  TATTTAACGTTGAATGTTACTGTTGTCGTCAAGATGGCATAAGACC...         tggcat   \n",
      "3  AAAAAGCAGTATTTCGGCGAGTAGCGCAGCTTGGTAGCGCAACTGG...         tggtag   \n",
      "4  TAGCCTCCGCCGTTTATGCACTTTTATCACTGGCTGGCACGAACCC...         tggcac   \n",
      "\n",
      "  PromoterMotifb                                   BriefDescription  \\\n",
      "0          tgccg  This promoter was identified by microarray ass...   \n",
      "1          ggcga  Transcription of acs may be negatively regulat...   \n",
      "2          tgcat  We assigned a putative transcription start sit...   \n",
      "3          tggtt  This promoter was identified by microarray ass...   \n",
      "4          tgcaa  The expression of this promoter is induced in ...   \n",
      "\n",
      "                   Reference  \\\n",
      "0                   14529615   \n",
      "1          14563880,10894724   \n",
      "2  10536136,11528004,3118148   \n",
      "3                   19969540   \n",
      "4           9696780,12003934   \n",
      "\n",
      "                                       OneHotEncoded  \n",
      "0  [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...  \n",
      "1  [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, ...  \n",
      "2  [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...  \n",
      "3  [1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, ...  \n",
      "4  [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, ...  \n"
     ]
    }
   ],
   "source": [
    "# Load CSV file\n",
    "promoter_df = pd.read_csv('/Users/bindusamba/Documents/GitHub/Master-s-project/csv/Promoter.csv')\n",
    "\n",
    "# Step 1: Clean the Promoter Sequences\n",
    "# Remove NaN values\n",
    "promoter_df = promoter_df.dropna(subset=[\"PromoterSequence\"])\n",
    "\n",
    "# Ensure sequences are strings and uppercase\n",
    "promoter_df[\"PromoterSequence\"] = promoter_df[\"PromoterSequence\"].astype(str).str.upper()\n",
    "\n",
    "# Ensure sequences are exactly 81 base pairs long (truncate or pad with 'N')\n",
    "def fix_sequence(seq, length=81):\n",
    "    seq = seq[:length]  # Truncate if too long\n",
    "    seq = seq.ljust(length, 'N')  # Pad with 'N' if too short\n",
    "    return seq\n",
    "\n",
    "promoter_df[\"PromoterSequence\"] = promoter_df[\"PromoterSequence\"].apply(fix_sequence)\n",
    "\n",
    "# Step 2: One-Hot Encoding Function\n",
    "def one_hot_encode(seq):\n",
    "    seq = seq.upper()  # Ensure uppercase\n",
    "    mapping = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'T': [0, 0, 0, 1], 'N': [0, 0, 0, 0]}\n",
    "    return np.array([mapping.get(nuc, [0, 0, 0, 0]) for nuc in seq])\n",
    "\n",
    "# Step 3: Apply One-Hot Encoding to Each Row\n",
    "promoter_df[\"OneHotEncoded\"] = promoter_df[\"PromoterSequence\"].apply(lambda seq: one_hot_encode(seq).flatten().tolist())\n",
    "\n",
    "# Step 4: Save the Updated CSV File (Overwrite Existing)\n",
    "promoter_df.to_csv(\"promoter.csv\", index=False)\n",
    "\n",
    "print(\"✅ One-hot encoding added as a new column 'OneHotEncoded' in 'promoter.csv'\")\n",
    "print(\"First 5 rows of the updated DataFrame:\")\n",
    "print(promoter_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense, BatchNormalization, GlobalMaxPooling1D, GaussianNoise\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bindusamba/Documents/GitHub/Master-s-project/.venv/lib/python3.9/site-packages/keras/src/layers/regularization/gaussian_noise.py:29: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.4470 - loss: 1.4230 - val_accuracy: 0.5000 - val_loss: 0.6933 - learning_rate: 5.0000e-04\n",
      "Epoch 2/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4567 - loss: 1.4819 - val_accuracy: 0.5588 - val_loss: 0.6885 - learning_rate: 5.0000e-04\n",
      "Epoch 3/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5701 - loss: 0.9566 - val_accuracy: 0.6765 - val_loss: 0.6871 - learning_rate: 5.0000e-04\n",
      "Epoch 4/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4970 - loss: 0.8981 - val_accuracy: 0.6471 - val_loss: 0.6906 - learning_rate: 5.0000e-04\n",
      "Epoch 5/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5447 - loss: 0.9137 - val_accuracy: 0.6765 - val_loss: 0.6891 - learning_rate: 5.0000e-04\n",
      "Epoch 6/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5257 - loss: 0.8586 - val_accuracy: 0.7059 - val_loss: 0.6868 - learning_rate: 5.0000e-04\n",
      "Epoch 7/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6274 - loss: 0.6830 - val_accuracy: 0.7059 - val_loss: 0.6845 - learning_rate: 5.0000e-04\n",
      "Epoch 8/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5590 - loss: 0.6801 - val_accuracy: 0.7059 - val_loss: 0.6797 - learning_rate: 5.0000e-04\n",
      "Epoch 9/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6429 - loss: 0.6406 - val_accuracy: 0.7059 - val_loss: 0.6783 - learning_rate: 5.0000e-04\n",
      "Epoch 10/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4669 - loss: 0.7645 - val_accuracy: 0.7059 - val_loss: 0.6779 - learning_rate: 5.0000e-04\n",
      "Epoch 11/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6559 - loss: 0.6008 - val_accuracy: 0.7059 - val_loss: 0.6745 - learning_rate: 5.0000e-04\n",
      "Epoch 12/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5530 - loss: 0.6609 - val_accuracy: 0.7059 - val_loss: 0.6694 - learning_rate: 5.0000e-04\n",
      "Epoch 13/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6419 - loss: 0.6271 - val_accuracy: 0.7059 - val_loss: 0.6664 - learning_rate: 5.0000e-04\n",
      "Epoch 14/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6978 - loss: 0.5965 - val_accuracy: 0.7059 - val_loss: 0.6577 - learning_rate: 5.0000e-04\n",
      "Epoch 15/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6977 - loss: 0.5722 - val_accuracy: 0.7059 - val_loss: 0.6583 - learning_rate: 5.0000e-04\n",
      "Epoch 16/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6653 - loss: 0.6048 - val_accuracy: 0.6765 - val_loss: 0.6651 - learning_rate: 5.0000e-04\n",
      "Epoch 17/40\n",
      "\u001b[1m6/9\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6332 - loss: 0.5556\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6526 - loss: 0.5745 - val_accuracy: 0.6765 - val_loss: 0.6632 - learning_rate: 5.0000e-04\n",
      "Epoch 18/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6461 - loss: 0.6077 - val_accuracy: 0.6765 - val_loss: 0.6610 - learning_rate: 2.5000e-04\n",
      "Epoch 19/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7802 - loss: 0.4727 - val_accuracy: 0.6765 - val_loss: 0.6560 - learning_rate: 2.5000e-04\n",
      "Epoch 20/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7196 - loss: 0.5752 - val_accuracy: 0.6765 - val_loss: 0.6545 - learning_rate: 2.5000e-04\n",
      "Epoch 21/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7868 - loss: 0.4696 - val_accuracy: 0.6765 - val_loss: 0.6568 - learning_rate: 2.5000e-04\n",
      "Epoch 22/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7862 - loss: 0.4513 - val_accuracy: 0.6765 - val_loss: 0.6527 - learning_rate: 2.5000e-04\n",
      "Epoch 23/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7769 - loss: 0.4240 - val_accuracy: 0.7059 - val_loss: 0.6403 - learning_rate: 2.5000e-04\n",
      "Epoch 24/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8021 - loss: 0.4287 - val_accuracy: 0.7059 - val_loss: 0.6366 - learning_rate: 2.5000e-04\n",
      "Epoch 25/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7941 - loss: 0.3950 - val_accuracy: 0.7059 - val_loss: 0.6276 - learning_rate: 2.5000e-04\n",
      "Epoch 26/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8127 - loss: 0.4562 - val_accuracy: 0.7059 - val_loss: 0.6137 - learning_rate: 2.5000e-04\n",
      "Epoch 27/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8472 - loss: 0.3847 - val_accuracy: 0.7059 - val_loss: 0.6074 - learning_rate: 2.5000e-04\n",
      "Epoch 28/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8443 - loss: 0.3838 - val_accuracy: 0.7059 - val_loss: 0.6071 - learning_rate: 2.5000e-04\n",
      "Epoch 29/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8747 - loss: 0.3742 - val_accuracy: 0.7059 - val_loss: 0.6052 - learning_rate: 2.5000e-04\n",
      "Epoch 30/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8951 - loss: 0.3547 - val_accuracy: 0.7059 - val_loss: 0.6076 - learning_rate: 2.5000e-04\n",
      "Epoch 31/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8127 - loss: 0.3925 - val_accuracy: 0.7059 - val_loss: 0.6046 - learning_rate: 2.5000e-04\n",
      "Epoch 32/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8519 - loss: 0.4054 - val_accuracy: 0.7059 - val_loss: 0.6079 - learning_rate: 2.5000e-04\n",
      "Epoch 33/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8082 - loss: 0.3941 - val_accuracy: 0.7059 - val_loss: 0.6107 - learning_rate: 2.5000e-04\n",
      "Epoch 34/40\n",
      "\u001b[1m6/9\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8990 - loss: 0.2613\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8826 - loss: 0.2852 - val_accuracy: 0.7059 - val_loss: 0.6151 - learning_rate: 2.5000e-04\n",
      "Epoch 35/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9001 - loss: 0.3091 - val_accuracy: 0.7059 - val_loss: 0.6190 - learning_rate: 1.2500e-04\n",
      "Epoch 36/40\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9147 - loss: 0.2833 - val_accuracy: 0.7059 - val_loss: 0.6234 - learning_rate: 1.2500e-04\n",
      "Epoch 37/40\n",
      "\u001b[1m5/9\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8869 - loss: 0.2690\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8968 - loss: 0.2652 - val_accuracy: 0.7059 - val_loss: 0.6265 - learning_rate: 1.2500e-04\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bindusamba/Documents/GitHub/Master-s-project/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/bindusamba/Documents/GitHub/Master-s-project/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/bindusamba/Documents/GitHub/Master-s-project/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 CNN Results\n",
      "Accuracy: 0.5952380952380952\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        17\n",
      "           1       0.60      1.00      0.75        25\n",
      "\n",
      "    accuracy                           0.60        42\n",
      "   macro avg       0.30      0.50      0.37        42\n",
      "weighted avg       0.35      0.60      0.44        42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# === Load and preprocess ===\n",
    "df = pd.read_csv(\"promoter.csv\")\n",
    "df[\"OneHotEncoded\"] = df[\"OneHotEncoded\"].apply(ast.literal_eval)\n",
    "df[\"Label\"] = (df[\"PromoterStrand\"] == \"+\").astype(int)\n",
    "\n",
    "# === Features ===\n",
    "X_flat = np.array(df[\"OneHotEncoded\"].tolist())\n",
    "X_seq = X_flat.reshape(len(X_flat), 81, 4)\n",
    "y = df[\"Label\"].values\n",
    "\n",
    "# === Train/test split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# === Class weights to handle imbalance ===\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# === Improved CNN model ===\n",
    "def build_cnn():\n",
    "    model = Sequential([\n",
    "        GaussianNoise(0.1, input_shape=(81, 4)),\n",
    "        Conv1D(128, 7, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Conv1D(256, 5, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Conv1D(512, 3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        GlobalMaxPooling1D(),\n",
    "\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# === Callbacks for training optimization ===\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "\n",
    "# === Build & Train ===\n",
    "cnn = build_cnn()\n",
    "history = cnn.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=40,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[reduce_lr, early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# === Evaluate ===\n",
    "y_pred = cnn.predict(X_test).round()\n",
    "print(\"\\n📊 CNN Results\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "cnn.save(\"cnn_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bindusamba/Documents/GitHub/Master-s-project/.venv/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Accuracy  Precision_Pos  Recall_Pos  F1_Pos\n",
      "MLP               50.00           0.57        0.68    0.62\n",
      "Random Forest     52.38           0.57        0.84    0.68\n",
      "AdaBoost          54.76           0.61        0.68    0.64\n",
      "SVM               57.14           0.59        0.96    0.73\n",
      "XGBoost           59.52           0.62        0.80    0.70\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Flatten CNN input shape (81x4 → 324)\n",
    "X_flat = X_seq.reshape(len(X_seq), -1)\n",
    "\n",
    "# Train-test split\n",
    "X_train_ml, X_test_ml, y_train_ml, y_test_ml = train_test_split(X_flat, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_ml, y_train_ml)\n",
    "    y_pred = model.predict(X_test_ml)\n",
    "    acc = accuracy_score(y_test_ml, y_pred)\n",
    "    report = classification_report(y_test_ml, y_pred, output_dict=True)\n",
    "    results[name] = {\n",
    "        \"Accuracy\": round(acc * 100, 2),\n",
    "        \"Precision_Pos\": round(report[\"1\"][\"precision\"], 2),\n",
    "        \"Recall_Pos\": round(report[\"1\"][\"recall\"], 2),\n",
    "        \"F1_Pos\": round(report[\"1\"][\"f1-score\"], 2)\n",
    "    }\n",
    "\n",
    "# Display comparison\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Labels: 57\n",
      "PromoterMotifa\n",
      "tggcac    74\n",
      "tggcat    36\n",
      "tggcaa     9\n",
      "tggcgt     8\n",
      "tggcgc     6\n",
      "Name: count, dtype: int64\n",
      "Epoch 1/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4247 - loss: 1.5028 - val_accuracy: 0.8182 - val_loss: 0.8270\n",
      "Epoch 2/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5037 - loss: 1.3043 - val_accuracy: 0.8182 - val_loss: 0.6380\n",
      "Epoch 3/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4287 - loss: 1.3059 - val_accuracy: 0.8182 - val_loss: 0.8350\n",
      "Epoch 4/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5512 - loss: 1.1623 - val_accuracy: 0.8182 - val_loss: 0.7983\n",
      "Epoch 5/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5617 - loss: 1.1395 - val_accuracy: 0.8182 - val_loss: 0.7049\n",
      "Epoch 6/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5788 - loss: 1.0607 - val_accuracy: 0.8182 - val_loss: 0.6781\n",
      "Epoch 7/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5969 - loss: 1.0345 - val_accuracy: 0.7273 - val_loss: 0.7100\n",
      "Epoch 8/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6870 - loss: 0.9868 - val_accuracy: 0.7273 - val_loss: 0.6394\n",
      "Epoch 9/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7178 - loss: 0.7577 - val_accuracy: 0.7273 - val_loss: 0.6381\n",
      "Epoch 10/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7225 - loss: 0.7522 - val_accuracy: 0.7273 - val_loss: 0.6899\n",
      "Epoch 11/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8109 - loss: 0.6501 - val_accuracy: 0.7273 - val_loss: 0.5799\n",
      "Epoch 12/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8421 - loss: 0.5169 - val_accuracy: 0.7273 - val_loss: 0.5730\n",
      "Epoch 13/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8086 - loss: 0.4316 - val_accuracy: 0.7273 - val_loss: 0.6135\n",
      "Epoch 14/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9377 - loss: 0.2837 - val_accuracy: 0.7273 - val_loss: 0.6371\n",
      "Epoch 15/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9415 - loss: 0.2320 - val_accuracy: 0.7273 - val_loss: 0.6791\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6296 - loss: 1.1975\n",
      "✅ Fixed Accuracy: 0.6296\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load and inspect Promoter.csv\n",
    "df = pd.read_csv('/Users/bindusamba/Documents/GitHub/Master-s-project/csv/Promoter.csv')\n",
    "\n",
    "# Drop any row with missing PromoterSequence or Motifa\n",
    "df = df.dropna(subset=[\"PromoterSequence\", \"PromoterMotifa\"])\n",
    "\n",
    "# Check label cardinality\n",
    "label_counts = df[\"PromoterMotifa\"].value_counts()\n",
    "print(\"Unique Labels:\", len(label_counts))\n",
    "print(label_counts.head())\n",
    "\n",
    "# Filter to top 5 frequent labels (optional, for sanity)\n",
    "top_labels = label_counts.head(5).index.tolist()\n",
    "df = df[df[\"PromoterMotifa\"].isin(top_labels)]\n",
    "\n",
    "# Prepare sequences and labels\n",
    "sequences = df[\"PromoterSequence\"].astype(str).str.upper().tolist()\n",
    "labels = df[\"PromoterMotifa\"].astype(str).tolist()\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "\n",
    "# Tokenize promoter sequences at character level\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "X = tokenizer.texts_to_sequences(sequences)\n",
    "X = pad_sequences(X, maxlen=100, padding='post')\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Input(shape=(100,)),\n",
    "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=32),\n",
    "    Conv1D(64, kernel_size=5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(len(np.unique(y)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile and train\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=15, batch_size=16, validation_split=0.1)\n",
    "\n",
    "# Evaluate\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(\"✅ Fixed Accuracy:\", round(acc, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Motif summary saved as 'motif_counts.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/Users/bindusamba/Documents/GitHub/Master-s-project/csv/Promoter.csv')\n",
    "\n",
    "# Drop rows with missing sequences or motifs\n",
    "df = df.dropna(subset=[\"PromoterSequence\", \"PromoterMotifa\"])\n",
    "\n",
    "# Count frequency of each motif\n",
    "motif_counts = df[\"PromoterMotifa\"].value_counts().reset_index()\n",
    "motif_counts.columns = [\"Motif\", \"Count\"]\n",
    "\n",
    "# Save to CSV\n",
    "motif_counts.to_csv(\"motif_counts.csv\", index=False)\n",
    "\n",
    "print(\"✅ Motif summary saved as 'motif_counts.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences: 209\n",
      "Total motif classes: 57\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Use all 210 sequences and motifs\n",
    "sequences = df[\"PromoterSequence\"].astype(str).str.upper().tolist()\n",
    "labels = df[\"PromoterMotifa\"].astype(str).tolist()\n",
    "\n",
    "# Encode motif labels as integers (0 to N-1)\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "\n",
    "# Optional: Check number of classes\n",
    "print(\"Total sequences:\", len(sequences))\n",
    "print(\"Total motif classes:\", len(np.unique(y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "6/6 - 0s - 40ms/step - accuracy: 0.9263 - loss: 0.5434 - val_accuracy: 0.7273 - val_loss: 0.7400\n",
      "Epoch 2/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 0.9789 - loss: 0.3090 - val_accuracy: 0.7273 - val_loss: 0.8102\n",
      "Epoch 3/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 0.9895 - loss: 0.1931 - val_accuracy: 0.7273 - val_loss: 0.8299\n",
      "Epoch 4/40\n",
      "6/6 - 0s - 7ms/step - accuracy: 0.9789 - loss: 0.2499 - val_accuracy: 0.7273 - val_loss: 0.9411\n",
      "Epoch 5/40\n",
      "6/6 - 0s - 7ms/step - accuracy: 1.0000 - loss: 0.1164 - val_accuracy: 0.7273 - val_loss: 0.9341\n",
      "Epoch 6/40\n",
      "6/6 - 0s - 7ms/step - accuracy: 1.0000 - loss: 0.1004 - val_accuracy: 0.7273 - val_loss: 0.9383\n",
      "Epoch 7/40\n",
      "6/6 - 0s - 7ms/step - accuracy: 0.9684 - loss: 0.1608 - val_accuracy: 0.7273 - val_loss: 1.0482\n",
      "Epoch 8/40\n",
      "6/6 - 0s - 7ms/step - accuracy: 0.9895 - loss: 0.1542 - val_accuracy: 0.7273 - val_loss: 1.0005\n",
      "Epoch 9/40\n",
      "6/6 - 0s - 7ms/step - accuracy: 0.9895 - loss: 0.0313 - val_accuracy: 0.7273 - val_loss: 0.9114\n",
      "Epoch 10/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 1.0000 - loss: 0.0464 - val_accuracy: 0.7273 - val_loss: 0.9444\n",
      "Epoch 11/40\n",
      "6/6 - 0s - 7ms/step - accuracy: 0.9895 - loss: 0.0617 - val_accuracy: 0.7273 - val_loss: 1.0279\n",
      "Epoch 12/40\n",
      "6/6 - 0s - 7ms/step - accuracy: 1.0000 - loss: 0.0645 - val_accuracy: 0.7273 - val_loss: 1.0673\n",
      "Epoch 13/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 1.0000 - loss: 0.0256 - val_accuracy: 0.7273 - val_loss: 1.0748\n",
      "Epoch 14/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 0.9579 - loss: 0.0521 - val_accuracy: 0.7273 - val_loss: 1.1105\n",
      "Epoch 15/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 1.0000 - loss: 0.0244 - val_accuracy: 0.7273 - val_loss: 1.1853\n",
      "Epoch 16/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 0.9895 - loss: 0.0219 - val_accuracy: 0.7273 - val_loss: 1.1730\n",
      "Epoch 17/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 1.0000 - loss: 0.0343 - val_accuracy: 0.7273 - val_loss: 1.2099\n",
      "Epoch 18/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 1.0000 - loss: 0.0192 - val_accuracy: 0.7273 - val_loss: 1.2961\n",
      "Epoch 19/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 0.9895 - loss: 0.0504 - val_accuracy: 0.7273 - val_loss: 1.2371\n",
      "Epoch 20/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 0.9895 - loss: 0.0206 - val_accuracy: 0.7273 - val_loss: 1.2169\n",
      "Epoch 21/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 1.0000 - loss: 0.0240 - val_accuracy: 0.7273 - val_loss: 1.1887\n",
      "Epoch 22/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 0.9895 - loss: 0.0186 - val_accuracy: 0.8182 - val_loss: 1.1634\n",
      "Epoch 23/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 1.0000 - loss: 0.0085 - val_accuracy: 0.7273 - val_loss: 1.1977\n",
      "Epoch 24/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 1.0000 - loss: 0.0185 - val_accuracy: 0.7273 - val_loss: 1.3528\n",
      "Epoch 25/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 0.9895 - loss: 0.0339 - val_accuracy: 0.7273 - val_loss: 1.4995\n",
      "Epoch 26/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 1.0000 - loss: 0.0320 - val_accuracy: 0.7273 - val_loss: 1.6331\n",
      "Epoch 27/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 0.9895 - loss: 0.0294 - val_accuracy: 0.7273 - val_loss: 1.7440\n",
      "Epoch 28/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 1.0000 - loss: 0.0347 - val_accuracy: 0.7273 - val_loss: 1.7697\n",
      "Epoch 29/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 1.0000 - loss: 0.0137 - val_accuracy: 0.7273 - val_loss: 1.6707\n",
      "Epoch 30/40\n",
      "6/6 - 0s - 9ms/step - accuracy: 1.0000 - loss: 0.0075 - val_accuracy: 0.7273 - val_loss: 1.5124\n",
      "Epoch 31/40\n",
      "6/6 - 0s - 9ms/step - accuracy: 1.0000 - loss: 0.0139 - val_accuracy: 0.7273 - val_loss: 1.4140\n",
      "Epoch 32/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 0.9895 - loss: 0.0060 - val_accuracy: 0.7273 - val_loss: 1.4155\n",
      "Epoch 33/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 0.9895 - loss: 0.0107 - val_accuracy: 0.7273 - val_loss: 1.4736\n",
      "Epoch 34/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 1.0000 - loss: 0.0058 - val_accuracy: 0.7273 - val_loss: 1.5507\n",
      "Epoch 35/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 1.0000 - loss: 0.0166 - val_accuracy: 0.7273 - val_loss: 1.5865\n",
      "Epoch 36/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 1.0000 - loss: 0.0060 - val_accuracy: 0.7273 - val_loss: 1.5559\n",
      "Epoch 37/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 1.0000 - loss: 0.0207 - val_accuracy: 0.7273 - val_loss: 1.5818\n",
      "Epoch 38/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 1.0000 - loss: 0.0075 - val_accuracy: 0.7273 - val_loss: 1.5803\n",
      "Epoch 39/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 1.0000 - loss: 0.0218 - val_accuracy: 0.7273 - val_loss: 1.5135\n",
      "Epoch 40/40\n",
      "6/6 - 0s - 8ms/step - accuracy: 1.0000 - loss: 0.0328 - val_accuracy: 0.7273 - val_loss: 1.3567\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5556 - loss: 2.1329\n",
      "✅ CNN Accuracy with Class Weights: 0.5556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute class weights to balance rare vs frequent motifs\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Re-train model using class weights\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=40,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1,\n",
    "    class_weight=class_weights_dict,  # 🔥 this is the key line\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Re-evaluate\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(\"✅ CNN Accuracy with Class Weights:\", round(acc, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Stratified split failed. Falling back to regular split.\n",
      "Epoch 1/30\n",
      "10/10 - 0s - 49ms/step - accuracy: 0.2467 - loss: 10.1453 - val_accuracy: 0.4706 - val_loss: 3.1915\n",
      "Epoch 2/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.3533 - loss: 8.1956 - val_accuracy: 0.4706 - val_loss: 2.9502\n",
      "Epoch 3/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.3600 - loss: 7.4936 - val_accuracy: 0.4706 - val_loss: 2.9846\n",
      "Epoch 4/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.2800 - loss: 7.2476 - val_accuracy: 0.4706 - val_loss: 3.0081\n",
      "Epoch 5/30\n",
      "10/10 - 0s - 5ms/step - accuracy: 0.3067 - loss: 7.0225 - val_accuracy: 0.4706 - val_loss: 2.9158\n",
      "Epoch 6/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.3467 - loss: 6.7651 - val_accuracy: 0.4706 - val_loss: 2.9780\n",
      "Epoch 7/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.3667 - loss: 6.6439 - val_accuracy: 0.4706 - val_loss: 2.9936\n",
      "Epoch 8/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.3800 - loss: 6.3404 - val_accuracy: 0.4706 - val_loss: 2.9969\n",
      "Epoch 9/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.4133 - loss: 5.7784 - val_accuracy: 0.4706 - val_loss: 3.0206\n",
      "Epoch 10/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.4333 - loss: 5.3733 - val_accuracy: 0.4706 - val_loss: 2.9846\n",
      "Epoch 11/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.4133 - loss: 5.0799 - val_accuracy: 0.4118 - val_loss: 3.0809\n",
      "Epoch 12/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.4467 - loss: 4.6356 - val_accuracy: 0.4118 - val_loss: 2.8932\n",
      "Epoch 13/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.4600 - loss: 4.0828 - val_accuracy: 0.4706 - val_loss: 3.0285\n",
      "Epoch 14/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.5067 - loss: 3.6327 - val_accuracy: 0.4706 - val_loss: 3.1169\n",
      "Epoch 15/30\n",
      "10/10 - 0s - 7ms/step - accuracy: 0.5267 - loss: 3.4043 - val_accuracy: 0.4706 - val_loss: 3.1989\n",
      "Epoch 16/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.5267 - loss: 3.1463 - val_accuracy: 0.4118 - val_loss: 3.2208\n",
      "Epoch 17/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.5867 - loss: 2.7169 - val_accuracy: 0.4118 - val_loss: 3.3251\n",
      "Epoch 18/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.6133 - loss: 2.4360 - val_accuracy: 0.4118 - val_loss: 3.3995\n",
      "Epoch 19/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.6067 - loss: 2.4050 - val_accuracy: 0.4118 - val_loss: 3.2087\n",
      "Epoch 20/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.6667 - loss: 2.0871 - val_accuracy: 0.4706 - val_loss: 3.7815\n",
      "Epoch 21/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.6067 - loss: 2.1354 - val_accuracy: 0.4706 - val_loss: 3.6352\n",
      "Epoch 22/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.7133 - loss: 1.7387 - val_accuracy: 0.4706 - val_loss: 3.9170\n",
      "Epoch 23/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.6667 - loss: 1.8404 - val_accuracy: 0.4706 - val_loss: 4.1665\n",
      "Epoch 24/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.7067 - loss: 1.6751 - val_accuracy: 0.4706 - val_loss: 3.8174\n",
      "Epoch 25/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.7267 - loss: 1.2384 - val_accuracy: 0.4118 - val_loss: 4.2170\n",
      "Epoch 26/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.7133 - loss: 1.4432 - val_accuracy: 0.4706 - val_loss: 4.5635\n",
      "Epoch 27/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.7533 - loss: 1.2527 - val_accuracy: 0.4706 - val_loss: 3.8076\n",
      "Epoch 28/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.7333 - loss: 1.2338 - val_accuracy: 0.4706 - val_loss: 4.2321\n",
      "Epoch 29/30\n",
      "10/10 - 0s - 10ms/step - accuracy: 0.7933 - loss: 0.9011 - val_accuracy: 0.4118 - val_loss: 4.4929\n",
      "Epoch 30/30\n",
      "10/10 - 0s - 6ms/step - accuracy: 0.7933 - loss: 0.9885 - val_accuracy: 0.4118 - val_loss: 4.8864\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4683 - loss: 4.5572\n",
      "✅ CNN Accuracy (All 58 Motifs — No Filtering): 0.4524\n"
     ]
    }
   ],
   "source": [
    "# use all 210 sequences + 58 motifs \n",
    "\n",
    "\n",
    "#  Load promoter dataset\n",
    "df = pd.read_csv('/Users/bindusamba/Documents/GitHub/Master-s-project/csv/Promoter.csv')\n",
    "df = df.dropna(subset=[\"PromoterSequence\", \"PromoterMotifa\"])\n",
    "\n",
    "\n",
    "#  Extract sequences and labels\n",
    "sequences = df[\"PromoterSequence\"].astype(str).str.upper().tolist()\n",
    "labels = df[\"PromoterMotifa\"].astype(str).tolist()\n",
    "\n",
    "# Encode motifs\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "#  Tokenize sequences\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "X = tokenizer.texts_to_sequences(sequences)\n",
    "X = pad_sequences(X, maxlen=100, padding='post')\n",
    "\n",
    "#  Stratified split – this may break if any motif only appears once!\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(\"⚠️ Stratified split failed. Falling back to regular split.\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "#  Compute class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "#  Build CNN\n",
    "model = Sequential([\n",
    "    Input(shape=(100,)),\n",
    "    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=32),\n",
    "    Conv1D(64, kernel_size=5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#  Train with class weights\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1,\n",
    "    class_weight=class_weights_dict,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "#  Evaluate\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(\"✅ CNN Accuracy (All 58 Motifs — No Filtering):\", round(acc, 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SVM Accuracy: 0.3143\n",
      "✅ Random Forest Accuracy: 0.5143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bindusamba/Documents/GitHub/Master-s-project/.venv/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MLP Accuracy: 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bindusamba/Documents/GitHub/Master-s-project/.venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ XGBoost Accuracy: 0.4286\n",
      "✅ AdaBoost Accuracy: 0.4571\n",
      "📁 Model results saved to: motif_model_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from xgboost import XGBClassifier\n",
    "#  Load and clean promoter data\n",
    "df = pd.read_csv('/Users/bindusamba/Documents/GitHub/Master-s-project/csv/Promoter.csv')\n",
    "df = df.dropna(subset=[\"PromoterSequence\", \"PromoterMotifa\"])\n",
    "\n",
    "#  Filter motifs that appear at least twice\n",
    "motif_counts = df[\"PromoterMotifa\"].value_counts()\n",
    "valid_motifs = motif_counts[motif_counts > 1].index\n",
    "df = df[df[\"PromoterMotifa\"].isin(valid_motifs)]\n",
    "\n",
    "#  Prepare sequences and labels\n",
    "sequences = df[\"PromoterSequence\"].astype(str).str.upper().tolist()\n",
    "labels = df[\"PromoterMotifa\"].astype(str).tolist()\n",
    "\n",
    "#  Encode motif labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "\n",
    "#  Extract 2-mer features\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 2), max_features=500)\n",
    "X = vectorizer.fit_transform(sequences).toarray()\n",
    "\n",
    "#  Stratified train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "#  Define models\n",
    "models = {\n",
    "    \"SVM\": SVC(kernel='linear', probability=True),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=300, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, use_label_encoder=False, eval_metric='mlogloss'),\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "#  Train and evaluate all models\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results.append((name, round(acc, 4)))\n",
    "    print(f\"✅ {name} Accuracy: {round(acc, 4)}\")\n",
    "\n",
    "#  Save accuracy results\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\"])\n",
    "results_df.to_csv(\"motif_model_comparison.csv\", index=False)\n",
    "print(\"📁 Model results saved to: motif_model_comparison.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
